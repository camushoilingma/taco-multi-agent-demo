services:
  vllm-model1:
    image: vllm/vllm-openai:latest
    runtime: nvidia
    ports:
      - "8081:8081"
    volumes:
      - model-cache:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN:-}
    command: >
      --model cyankiwi/Qwen3-VL-8B-Instruct-AWQ-4bit
      --dtype half
      --quantization awq
      --max-model-len 2048
      --port 8081
      --served-model-name qwen3-vl-8b
      --api-key demo-key
      --gpu-memory-utilization 0.45
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8081/health"]
      interval: 15s
      timeout: 10s
      retries: 20
      start_period: 180s
    restart: unless-stopped

  vllm-model2:
    image: vllm/vllm-openai:latest
    runtime: nvidia
    ports:
      - "8082:8082"
    volumes:
      - model-cache:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN:-}
    command: >
      --model Qwen/Qwen2.5-VL-7B-Instruct-AWQ
      --dtype half
      --quantization awq
      --max-model-len 2048
      --port 8082
      --served-model-name qwen2.5-vl-7b
      --api-key demo-key
      --gpu-memory-utilization 0.45
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8082/health"]
      interval: 15s
      timeout: 10s
      retries: 20
      start_period: 180s
    restart: unless-stopped

  backend:
    build: ./backend
    ports:
      - "8000:8000"
    environment:
      - MODEL1_BASE_URL=http://vllm-model1:8081/v1
      - MODEL1_API_KEY=demo-key
      - MODEL1_NAME=qwen3-vl-8b
      - MODEL2_BASE_URL=http://vllm-model2:8082/v1
      - MODEL2_API_KEY=demo-key
      - MODEL2_NAME=qwen2.5-vl-7b
      - MOCK_LLM=${MOCK_LLM:-false}
      - DEBUG_MODE=true
    depends_on:
      vllm-model1:
        condition: service_healthy
      vllm-model2:
        condition: service_healthy
    restart: unless-stopped

  frontend:
    build: ./frontend
    ports:
      - "3000:3000"
    depends_on:
      - backend
    restart: unless-stopped

volumes:
  model-cache:
